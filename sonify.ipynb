{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EvalWhisper import *\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import statistics\n",
    "\n",
    "from EvalWhisper import EvalWhisper\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model . . . (openai/whisper-large)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "model_size = \"large\"\n",
    "model_checkpoint = f\"openai/whisper-{model_size}\"\n",
    "processor_checkpoint = [f\"openai/whisper-{model_size}\"]\n",
    "#load processor and model\n",
    "print(f\"Loading model . . . ({model_checkpoint})\")\n",
    "whisper_evaluator = EvalWhisper(model_checkpoint, *processor_checkpoint)\n",
    "print(f\"Loaded model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"librispeech_asr\", split=\"validation.clean\", streaming=True)\n",
    "print(f\"Loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '2277-149896-0000.flac',\n",
       " 'audio': {'path': '2277-149896-0000.flac',\n",
       "  'array': array([ 0.00186157,  0.0005188 ,  0.00024414, ..., -0.00097656,\n",
       "         -0.00109863, -0.00146484]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': \"HE WAS IN A FEVERED STATE OF MIND OWING TO THE BLIGHT HIS WIFE'S ACTION THREATENED TO CAST UPON HIS ENTIRE FUTURE\",\n",
       " 'speaker_id': 2277,\n",
       " 'chapter_id': 149896,\n",
       " 'id': '2277-149896-0000'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ds.take(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(output_path, whisper_evaluator, inputs, mask_ratios, mode_value, what_to_mask_list, sampling_rate=16000):\n",
    "    output_audio_list = []\n",
    "    \n",
    "    \n",
    "    for idx, input in tqdm(enumerate(inputs)):\n",
    "        masked_spectrograms = []\n",
    "        original_spectrogram = whisper_evaluator.get_spectrogram(input)\n",
    "        # print(f\"os:{original_spectrogram.shape}\")\n",
    "        sf.write(f\"{output_path}/{idx}_original_all.wav\", input[\"audio\"][\"array\"], sampling_rate) #whisper_evaluator.top_r_features(input, r=1.0, mode=\"retain\", where=\"top\").detach().numpy()\n",
    "        sf.write(f\"{output_path}/{idx}_{1.0}_all.wav\", whisper_evaluator.sonify(original_spectrogram), sampling_rate) #whisper_evaluator.top_r_features(input, r=1.0, mode=\"retain\", where=\"top\").detach().numpy()\n",
    "        with open(f\"{output_path}/{idx}_original_transcription.txt\", \"w\") as op_file:\n",
    "            op_file.write(input[\"text\"] + \"\\n\")\n",
    "        break\n",
    "        for mask_ratio in tqdm(mask_ratios):\n",
    "            mask_list = []\n",
    "            for mask in what_to_mask_list:\n",
    "                masked_spectrogram = whisper_evaluator.top_r_features(input, r=mask_ratio, mode=mode_value, where=mask)\n",
    "                masked_audio = whisper_evaluator.sonify(masked_spectrogram.detach().numpy())\n",
    "                mask_list.append(masked_audio)\n",
    "                sf.write(f\"{output_path}/{idx}_{mask_ratio}_{mask}.wav\", masked_audio, sampling_rate)\n",
    "            masked_spectrograms.append(mask_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:31, 10.40s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_audio() missing 1 required positional argument: 'sampling_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/leakycauldron/Desktop/salASR/sonify.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leakycauldron/Desktop/salASR/sonify.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m tqdm(ds\u001b[39m.\u001b[39mskip(skip_to_index)\u001b[39m.\u001b[39mtake(num_samples)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/leakycauldron/Desktop/salASR/sonify.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     inputs\u001b[39m.\u001b[39mappend(sample)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/leakycauldron/Desktop/salASR/sonify.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m get_audio(output_path, whisper_evaluator, inputs, mask_ratios, mode_value, what_to_mask_list)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_audio() missing 1 required positional argument: 'sampling_rate'"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"./output_audio_new\")\n",
    "pathlib.Path.mkdir(output_path, exist_ok=True)\n",
    "mask_ratios = [0.8, 0.6, 0.4, 0.2]\n",
    "mode_value = \"retain\"\n",
    "what_to_mask_list = [\"top\", \"bottom\", \"random\"]\n",
    "num_samples = 3\n",
    "skip_to_index = 510\n",
    "inputs = []\n",
    "for sample in tqdm(ds.skip(skip_to_index).take(num_samples)):\n",
    "    inputs.append(sample)\n",
    "get_audio(output_path, whisper_evaluator, inputs, mask_ratios, mode_value, what_to_mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
